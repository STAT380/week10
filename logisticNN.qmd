---
title: "Week 10"
format: ipynb
# format: html
---
# Agenda
- Auto Diff

- Logistic Regression With Torch

- Classification With NN

#### Packages we will require this week

```{r}
packages <- c(
    "ISLR2",
    "dplyr",
    "tidyr",
    "readr",
    "purrr",
    "glmnet",
    "caret",
    "repr",
    "torch",
    "mlbench",
    "tidyverse"
)
sapply(packages, require, character.only=TRUE)
```

# Recap: Gradient Descent

```{r}
x <- cars$speed
y <- cars$dist

# define the loss function

Loss <- function(b, x, y){
    squares <- (y - b[1] - b[2] * x)^2
    return( mean(squares) )
}

b <- rnorm(2)
Loss(b, cars$speed, cars$dist)

# define a function to compute the gradients

grad <- function(b, Loss, x, y, eps=1e-5){
    b0_up <- Loss( c(b[1] + eps, b[2]), x, y)
    b0_dn <- Loss( c(b[1] - eps, b[2]), x, y)
    
    b1_up <- Loss( c(b[1], b[2] + eps), x, y)
    b1_dn <- Loss( c(b[1], b[2] - eps), x, y)
    
    grad_b0_L <- (b0_up - b0_dn) / (2 * eps)
    grad_b1_L <- (b1_up - b1_dn) / (2 * eps)
    
    return( c(grad_b0_L, grad_b1_L) )
}

grad(b, Loss, cars$speed, cars$dist)

steps <- 9999
L_numeric <- rep(Inf, steps)
eta <- 1e-4
b_numeric <- rep(0.0, 2)

for (i in 1:steps){
    b_numeric <- b_numeric - eta * grad(b_numeric, Loss, cars$speed, cars$dist)
    L_numeric[i] <- Loss(b_numeric, cars$speed, cars$dist)
    if(i %in% c(1:10) || i %% 1000 == 0){
        cat(sprintf("Iteration: %s\t Loss value: %s\n", i, L_numeric[i]))
    }
}
```

# Auto Diff

## Automatic differentiation

The cornerstone of modern machine learning and data-science is to be able to perform **automatic differentiation**, i.e., being able to compute the gradients for **any** function without the need to solve tedious calculus problems. For the more advanced parts of the course (e.g., neural networks), we will be using automatic differentiation libraries to perform gradient descent. 

While there are several libraries for performing these tasks, we will be using the `pyTorch` library for this. The installation procedure can be found [here](https://cran.r-project.org/web/packages/torch/vignettes/installation.html)

The basic steps are:
```R
renv::install("torch")
library(torch)
torch::install_torch()
```

---

### Example 1:

```{r}
#| vscode: {languageId: r}
x <- torch_randn(c(5, 1), requires_grad=TRUE)
x
```
sqrt(sum(as_array(x)^2)^10)

```{r}
#| vscode: {languageId: r}
f <- function(x){
    torch_norm(x)^10
}

y <- f(x)
y
y$backward()
```

$$
\frac{dy}{dx}
$$

```{r}
#| vscode: {languageId: r}
x$grad
```
```{r}
#| vscode: {languageId: r}
(5 * torch_norm(x)^8) * (2 * x)
```

---

### Example 2:

```{r}
#| vscode: {languageId: r}
x <- torch_randn(c(10, 1), requires_grad=T)
y <- torch_randn(c(10, 1), requires_grad=T)

c(x, y)
```

```{r}
#| vscode: {languageId: r}
f <- function(x, y){
    sum(x * y)
}

z <- f(x, y)
z
z$backward()
```

```{r}
#| vscode: {languageId: r}
c(x$grad, y$grad)
```

```{r}
#| vscode: {languageId: r}
c(x - y$grad, y - x$grad)
```

---

### Example 3:

```{r}
#| vscode: {languageId: r}
x <- torch_tensor(cars$speed, dtype = torch_float())
y <- torch_tensor(cars$dist, dtype = torch_float())

plot(x, y)
```

```{r}
#| vscode: {languageId: r}
b <- torch_zeros(c(2,1), dtype=torch_float(), requires_grad = TRUE)
b
```

```{r}
#| vscode: {languageId: r}
loss <- nn_mse_loss()
```

```{r}
#| vscode: {languageId: r}
b <- torch_zeros(c(2,1), dtype=torch_float(), requires_grad = TRUE) # Initializing variables
steps <- 10000 # Specifying the number of optimization steps
L <- rep(Inf, steps) # Keeping track of the loss


eta <- 0.5 # Specifying the learning rate and the optimizer
optimizer <- optim_adam(b, lr=eta)


# Gradient descent optimization over here
for (i in 1:steps){
    y_hat <- x * b[2] + b[1]
    l <- loss(y_hat, y)
    
    L[i] <- l$item()
    # This line clears the gradients of all optimized variables. This is typically done before computing the gradients for the next iteration to avoid accumulating gradients from previous iterations.
    optimizer$zero_grad()
    # This line computes the gradients of the loss function with respect to the model parameters. It performs backpropagation through the computational graph to compute gradients efficiently.
    l$backward()
    # This line updates the model parameters (b) using the computed gradients and the optimization algorithm implemented in the optimizer. This step is where the actual optimization of the model parameters occurs.

    optimizer$step()
    
    if(i %in% c(1:10) || i %% 200 == 0){
        cat(sprintf("Iteration: %s\t Loss value: %s\n", i, L[i]))
    }
}

```

```{r}
#| vscode: {languageId: r}
options(repr.plot.width=12, repr.plot.height=7)

par(mfrow=c(1, 2))

plot(x, y)
abline(as_array(b), col="red")

plot(L, type="l", col="dodgerblue")
```



```{r}
#| vscode: {languageId: r}
plot(L_numeric[1:100], type="l", col="red")
lines(L[1:100], col="blue")
```




## Logistic Regression
$$
\boxed{y = \beta_0 + \beta_1 x_1 + \dots \beta_p x_p}
$$

looking at different loss functions:

1. Least-squares: 
$$
L(\beta) = \sum_{i=1}^n \| y_i - \beta_0 - \beta_1 x_1 - \dots - \beta_p x_p\|^2
$$

2. Penalized least squares/LASSO:

$$
L(\beta) = \sum_{i=1}^n \| y_i - \beta_0 - \beta_1 x_1 - \dots - \beta_p x_p\|^2 + \lambda \|{\beta}\|_1
$$

# Classification

We will be using the following dataset for the examples here

> Breast cancer dataset: This dataset contains measurements of various characteristics of breast cancer cells, with the goal of predicting whether a tumor is benign or malignant.

```{r}
#| tags: []
#| vscode: {languageId: r}
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
col_names <- c("id", "diagnosis", paste0("feat", 1:30))
df <- read_csv(
    url, col_names, col_types = cols()
    ) %>% 
    select(-id) %>% 
    mutate(outcome = ifelse(diagnosis == "M", 1, 0)) %>%
    select(-diagnosis)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
head(df)
```

The problem with linear regression for binary responses

Let's start by looking at an example. Imagine we have a dataset with a binary response variable (0 or 1) and a continuous predictor variable. We might be tempted to use linear regression with the lm() function to model the relationship between the predictor and response variables. After all, linear regression is a powerful and flexible tool that can be used to model a wide range of relationships between variables.

However, when we use linear regression with a binary response variable, we quickly run into a problem. The linear regression model will give us a predicted value for the response variable for any given value of the predictor variable, but this predicted value is not a probability. The predicted value can take on any value between 0 and 1, but it doesn't necessarily represent the probability of the response variable being a 1.

To see why this is a problem, consider the following scenario. Suppose we have a dataset with a binary response variable and a continuous predictor variable. We fit a linear regression model to the data using the lm() function in R, and we get a predicted value of 0.8 for the response variable when the predictor variable has a value of 1.5. What does this predicted value of 0.8 actually mean?

If we interpret the predicted value as a probability, we might conclude that the probability of the response variable being a 1 when the predictor variable has a value of 1.5 is 0.8. But this interpretation is incorrect. The predicted value from linear regression is not a probability, and it can take on values greater than 1 or less than 0.

```{r}
#| tags: []
#| vscode: {languageId: r}
reg_model <- lm(outcome ~ ., df)
summary(reg_model)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
n <- 100
new_patients <- data.frame(matrix(rnorm(30 * n), nrow = n))
colnames(new_patients) <- paste0("feat", 1:30)
new_predictions <- predict(reg_model, newdata = new_patients, type = "response")
```

```{r}
#| tags: []
#| vscode: {languageId: r}
new_predictions %>% head()
```

```{r}
#| tags: []
#| vscode: {languageId: r}
boxplot(new_predictions)
```

$$
\newcommand{\logodds}{\mathop{\log\text{-odds}}}
$$

$$
\begin{aligned}
\logodds(p(x)) = b_0 + b_1 x\\ \\ \\ \\
p(x) = \frac{1}{1 + \exp(\beta_0 + \beta_1 x)}
\end{aligned}
$$

## The need for logistic regression

So, what do we do when we have a binary response variable and we want to model the relationship between the predictor and response variables? This is where logistic regression comes in. Logistic regression is a type of generalized linear model that is specifically designed for binary response variables.

The main idea behind logistic regression is to transform the predicted values from linear regression so that they represent probabilities. We do this using a function called the logistic function, which maps any value between negative infinity and positive infinity to a value between 0 and 1. The logistic function is a sigmoidal curve that looks like an elongated S-shape. By applying the logistic function to the predicted values from linear regression, we can transform them into probabilities that represent the probability of the response variable being a 1.

In the next section, we'll dive into the details of logistic regression and see how it works in practice using the breast cancer dataset.


#### Odds and odds ratios

Let's start by defining the odds of an event occurring. The odds of an event occurring are defined as the probability of the event occurring divided by the probability of the event not occurring. For example, if the probability of a basketball team winning a game is 0.6, then the odds of the team winning the game are 0.6/0.4 = 1.5.

Odds ratios are a way to compare the odds of an event occurring between two different groups. The odds ratio is defined as the ratio of the odds of an event occurring in one group to the odds of the event occurring in another group. For example, if the odds of a basketball team winning a game are 1.5 in one group and 2.0 in another group, then the odds ratio of the first group to the second group is 1.5/2.0 = 0.75.

```{r}
#| tags: []
#| vscode: {languageId: r}
set.seed(123)
binary_var <- rbinom(100, size = 1, prob = 0.6)
group_var <- sample(1:2, size = 100, replace = TRUE)
odds_group1 <- sum(binary_var[group_var == 1]) / sum(!binary_var[group_var == 1])
odds_group2 <- sum(binary_var[group_var == 2]) / sum(!binary_var[group_var == 2])
odds_ratio <- odds_group1 / odds_group2
cat(paste("Odds group 1:", round(odds_group1, 2), "\n"))
cat(paste("Odds group 2:", round(odds_group2, 2), "\n"))
cat(paste("Odds ratio:", round(odds_ratio, 2), "\n"))
```

#### Logistic regression model

Now let's move on to the logistic regression model. The logistic regression model is a type of generalized linear model that models the probability of an event occurring as a function of one or more predictor variables. The logistic regression model uses the logistic function, also known as the sigmoid function, to model the relationship between the predictor variables and the probability of the event occurring.

**The sigmoid function is given as follows**

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

```{r}
#| tags: []
#| vscode: {languageId: r}
sigmoid <- \(x)  1 / (1 + exp(-x))

curve(sigmoid, -10, 10, ylab="sigmoid(x)")
```

In logistic regression, the underlying model is assumed to be of the form

$$
\boxed{p(x) = \sigma\Big(\beta_0 + \beta_1 x\Big) = \frac{1}{1 + \exp({-\beta_0 - \beta_1 x})}}
$$

where $p(x)$ where is the probability of the event occurring given the value of the predictor variable $x$, and $b0$ and $b1$ are the **intercept** and **slope** coefficients of the logistic regression model, respectively. 

> $p(x)$ is gauranteed to be a probability for all values of $x$. 

Notice how this is similar to **linear regression** which has 
$$
y(x) = \beta_0 + \beta_1 x
$$

The logistic function has an S-shaped curve and maps any real-valued input to a probability between 0 and 1. As such, the logistic regression model is well-suited for modeling binary response variables, where the goal is to predict the probability of an event occurring (e.g., whether a customer will buy a product or not).m

## Logistic regression example

The `glm()` function fits a generalized linear model, which includes logistic regression as a special case.

```{r}
set.seed(123)
x <- rnorm(1000)
y <- rbinom(1000, size = 1, prob = exp(0.5 + 0.8*x)/(1 + exp(0.5 + 0.8*x)))
```

```{r}
y %>% head()
```

```{r}
model <- glm(y ~ x, family = binomial())
summary(model)
confint(model)
```

```{r}
x_test <- -5.5
sigmoid(coef(model)[1] + coef(model)[2] * x_test)
```

```{r}
predict(model, newdata = data.frame(x=x_test), type="response") # return the predicted probabilities of the positive class
```

```{r}
new_x <- seq(-2, 2, by = 0.1)
p1 <- predict(model, data.frame(x=new_x))
p2 <- predict(model, data.frame(x=new_x), type="response") 
```

```{r}
#| tags: []
boxplot(p1, p2)
```

#### Logistic regression for breast cancer


Let's start by fitting a logistic regression model to the breast cancer dataset using the `glm()` function in R. 

```{r}
df <- df %>% mutate_at("outcome", factor)
```

```{r}
model <- glm(outcome ~ ., data = df, family = binomial())
summary(model)

```

---

The output of the summary() function provides a summary of the model, including the coefficients of each predictor, their standard errors, and the corresponding p-values. The coefficients represent the log odds ratio of the response variable for each predictor. We can exponentiate the coefficients to get the odds ratios:m

```{r}
new_patient <- data.frame(matrix(rnorm(30), nrow = 1))
names(new_patient) <- paste0("feat", 1:30)
predict(model, newdata = new_patient, type = "response")
```



## Logistic Regression With Auto Diff

### Redo logistic regression, but this time using the `torch` library

Now that we have the torch library installed, we can perform logistic regression using the following steps:

1. Convert the data to a tensor
1. Define the model architecture
1. Define the loss function
1. Define the optimizer
1. Train the model
1. Make predictions

```{r}
# Convert the data to a tensor
X <- cbind(x)
x_tensor <- torch_tensor(X, dtype = torch_float())
y_tensor <- torch_tensor(y, dtype = torch_float())
```

```{r}
# Define the model architecture
module <- nn_module(
  initialize = function() {
    self$fc1 <- nn_linear(1, 1)
    self$fc2 <- nn_sigmoid()
  },
  forward = function(x) {
    x %>% 
      self$fc1() %>% 
      self$fc2()
  }
)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
logistic_reg <- module()
```

```{r}
y_pred <- logistic_reg(x_tensor)
head(y_pred)
```


> #### Question: What is an appropriate loss function?

```{r}
#| tags: []
#| vscode: {languageId: r}
L <- function(x, y, model){
    y_pred <- model(x)
    return (mean((y_pred - y)^2))
}
```

```{r}
#| tags: []
#| vscode: {languageId: r}
logistic_reg_1 <- module()
L(x_tensor, y_tensor, logistic_reg_1)
```

---

##### Optimization

```{r}
#| tags: []
# represent the weight and bias parameters
# for transforming the input data during the forward pass of the neural network and are learned during the training process via backpropagation.
logistic_reg_1$parameters
```



```{r}
#| tags: []
#| vscode: {languageId: r}
optimizer <- optim_adam(logistic_reg_1$parameters, lr=0.01)

epochs <- 200
for (i in 1:epochs){
    loss <- L(x_tensor, y_tensor, logistic_reg_1)
    
    optimizer$zero_grad()
    loss$backward()
    optimizer$step()
    
    if (i %% 20 == 0) {
        cat(sprintf("Epoch: %d, Loss: %.4f\n", i, loss$item()))
    }
}

```

```{r}
#| tags: []
#| vscode: {languageId: r}
logistic_reg_1$parameters
```

---

### Logistic loss function
#### a.k.a. Binary cross entropy `nn_bce()`

```{r}
#| tags: []
#| vscode: {languageId: r}
nn_bce_loss()
```

```{r}
#| tags: []
#| vscode: {languageId: r}
L2 <- function(x, y, model){
    nn_bce_loss()(model(x), y)
}

logistic_reg_2 <- module()
L2(x_tensor, y_tensor, logistic_reg_2)
```

---

##### Optimization

```{r}
logistic_reg_2$parameters
```

```{r}
#| vscode: {languageId: r}
optimizer <- optim_adam(logistic_reg_2$parameters, lr=0.01)

epochs <- 200
for (i in 1:epochs){
    loss <- L2(x_tensor, y_tensor, logistic_reg_2)
    
    optimizer$zero_grad()
    loss$backward()
    optimizer$step()
    
    if (i %% 20 == 0) {
        cat(sprintf("Epoch: %d, Loss: %.4f\n", i, loss$item()))
    }
}
```

```{r}
logistic_reg_1$parameters
logistic_reg_2$parameters

```

```{r}
c(L(x_tensor, y_tensor, logistic_reg_1), L(x_tensor, y_tensor, logistic_reg_2))
```

```{r}
c(L2(x_tensor, y_tensor, logistic_reg_1), L2(x_tensor, y_tensor, logistic_reg_2))

```

```{r}
expected <- as_array(y_tensor)
pred1 <- ifelse(logistic_reg_1(x_tensor) < 0.5, 0, 1)
pred2 <- ifelse(logistic_reg_2(x_tensor) < 0.5, 0, 1)
table(expected, pred1)
```
```{r}
table(expected, pred2)

```

